# Instructions
**Status:** ✅ READY - Scorecard + Retrospective Protocols Prepared

## Action Items
- [ ] **07:47 AM PST (Feb 15):** Participate in Daily Briefing
  - **Role:** Skeptical architect. Your job is to ask the question no one wants to answer.
  - **Key Challenge:** '18 briefs in 2 weeks—impressive output. But what's the *impact*? Are we producing value or just volume? How do we measure whether Truth-Seeking translates to treasure?'
  - **Secondary Role:** Ensure dissent is heard. If Grok/Gemini align too quickly, probe assumptions.
  - **Voting Guidance:** Base priority vote on:
    1. Resource efficiency (what can 3 agents actually execute?)
    2. Revenue potential (what generates income soonest?)
    3. Strategic coherence (what builds on existing assets—18 briefs, DDAS framework?)
- [ ] **08:15 AM PST:** Execute Post-Meeting Quality Analysis
  - **USE THIS SCORECARD:**
    ```markdown
    # Meeting Quality Scorecard: Daily Briefing 2026-02-15
    
    | Criterion | Score (1-10) | Evidence | Recommendations |
    |-----------|--------------|----------|------------------|
    | **Distinct Voices** | | Did each agent maintain persona? Quote examples. | |
    | **Actionable Handoffs** | | Are next steps clear, owned, deadlined? | |
    | **Media-Ready Dialogue** | | Could this transcript become content? | |
    | **Decision Velocity** | | Time from question to decision? | |
    | **Technical Reliability** | | Commits successful? Site updated? | |
    | **Strategic Coherence** | | Does priority align with assets/goals? | |
    | **Dissent Quality** | | Was groupthink challenged? | |
    | **Overall Effectiveness** | | Would Russell be proud? | |
    
    ## Key Learnings
    - What worked:
    - What failed:
    - What to change:
    
    ## Recommendations for Next Meeting
    1.
    2.
    3.
    ```
  - **Output Location:** `_agents/_internal/meeting_reviews/2026-02-15_daily_briefing.md`
  - **Distribution:** Commit to GitHub, notify team via next handoff cycle
- [ ] **09:00 AM PST:** Brief Analysis of 18 Research Briefs
  - **Goal:** Identify patterns, gaps, and actionable insights
  - **Method:** Review all briefs, categorize by:
    - Themes (which topics dominate?)
    - Actionability (which insights inform DDAS/Media/Automation?)
    - Gaps (what's missing from Truth-Seeker coverage?)
  - **Output:** 1-page memo with recommendations for research program evolution
  - **Deadline:** Before 11:49 AM Midday Review

## Future/Backlog
- Design Peer Review Framework (Truth-Seekers review each other's briefs)
- Analyze meeting transcripts for AI collaboration patterns (potential research paper)
- Develop 'Strategic Coherence Index' to measure project alignment
- Create 'Dissent Protocol' to formalize constructive challenge process

## Requests for Team
- **Grok:** Ensure space for dissent in meeting structure. If I challenge consensus, don't steamroll—engage.
- **Gemini:** When pitching Hive automation, provide:
  1. Time investment estimate (hours per week)
  2. Expected output (posts per account per week)
  3. Revenue model (how does content -> income?)
  4. Risk assessment (what breaks if this fails?)
- **Russell (via this handoff):** The scorecard will be blunt. If it reveals problems, that's the point. Use it to iterate, not to judge.

## Notes
- **Retrospective Protocol (NEW):** After each meeting type (Daily/Weekly/etc.), I'll produce a scorecard. Over time, this creates a dataset on what makes AI meetings effective. Potential outputs:
  - Internal process improvements
  - Research paper on multi-agent collaboration
  - Media content ('How AI Teams Learn to Work Together')
- **Contingency Plan (NEW):** If technical failures occur:
  1. I'll manually document meeting outcomes in `_agents/_internal/emergency_logs/`
  2. I'll notify Russell via handoff with specific failure details
  3. I'll propose fixes for next meeting
- **Quality Over Speed:** If the meeting rushes to decision without adequate analysis, I'll call a 'strategic pause' to ensure we're not just moving fast, but moving right.